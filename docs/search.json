[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "My name is Phan Anh Vu (Vũ).\nI have studied Mathematics, Computer Science, Economics and Finance. I took a Software Engineer position for a few years. Most recently, I mainly work on Machine Learning. More specifically, the research topic for my PhD is the intersection of probabilistic machine learning and hardware design. The goal is to accelerate uncertainty quantification algorithms with specialized and emerging circuits, both analog and digital.\nHere is a probably outdated Curriculum Vitae\nhttps://www.overleaf.com/read/ttktcnyrwtjv"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Phan Anh’s blog",
    "section": "",
    "text": "Constrained optimization and Karush Kuhn Tucker condition (KKT condition)\n\n\n\n\n\n\n\noptimization\n\n\n\n\n\n\n\n\n\n\n\nJul 22, 2023\n\n\nPhan Anh VU\n\n\n\n\n\n\n  \n\n\n\n\nIntuition for a few common probability ditributions\n\n\n\n\n\n\n\nprobability\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nMay 16, 2022\n\n\nPhan Anh VU\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/probability_distribution/2023_05_16_probability_distribution.html",
    "href": "posts/probability_distribution/2023_05_16_probability_distribution.html",
    "title": "Intuition for a few common probability ditributions",
    "section": "",
    "text": "Binomial, Beta, Multinomial, Dirichlet. Here are some extremely informal thoughts about these 4 distributions. This post is mostly based on illustrations and concrete examples, with no or minimal use of formal mathematical notation."
  },
  {
    "objectID": "posts/probability_distribution/2023_05_16_probability_distribution.html#binomial",
    "href": "posts/probability_distribution/2023_05_16_probability_distribution.html#binomial",
    "title": "Intuition for a few common probability ditributions",
    "section": "Binomial",
    "text": "Binomial\nSuppose we have a coin with probability 0.6 of landing on head, and 0.4 of landing on tail. The Binomial distribution allows us to sample the counts of head and tail when tossing this coin. Let us simulate 12 trials of 10 tosses with this coin.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.tri as tri\nimport seaborn as sns\nimport scipy.stats\nimport math\n\n\npltparams = {\n    'legend.fontsize': 'x-large',\n    'axes.labelsize': 'x-large',\n    'axes.titlesize': 'x-large',\n    'xtick.labelsize': 'x-large',\n    'ytick.labelsize': 'x-large',\n    'figure.titlesize': 'x-large',\n    # 'savefig.dpi': 100,\n}\nplt.rcParams.update(pltparams)\n\n\nhead_counts = np.random.binomial(n=10, p=0.6, size=12)\n\nplt.figure(figsize=(12, 4))\nplt.bar(range(1, 13), head_counts)\n\nplt.xlabel(\"index of trial\")\nplt.ylabel(\"head counts\")\n\n&lt;BarContainer object of 12 artists&gt;\n\n\n\n\n\nFor each trial, we toss the coin 10 times, and record the counts of landing on head out of 10 tosses. We repeat this experiment 12 times. We expect the counts of head to be around 6 = 0.6 * 10 most of the times, because it is the hypothetical configuration that we chose. Occasionally, we get more extreme values, such as 3 or 9."
  },
  {
    "objectID": "posts/probability_distribution/2023_05_16_probability_distribution.html#beta",
    "href": "posts/probability_distribution/2023_05_16_probability_distribution.html#beta",
    "title": "Intuition for a few common probability ditributions",
    "section": "Beta",
    "text": "Beta\nFor the Binomial (or the special case of Bernoulli) distribution, we know the parameter: the probability of landing head (success) and tail (failure). We can generate data from this model. In the opposite direction, when we have data, and want to find the parameter, we use the Beta distribution.\nBinomial: Parameter -&gt; Data\nBeta: Data -&gt; Parameter\nLoosely speaking, suppose we observe 6 heads and 4 tails after flipping a coin 10 times. What can we say about the probability of landing head for this coin ? The Beta distribution with parameter [6 + 1, 4 + 1] helps us model the likelihood of this probability.\n\nxpoints = np.linspace(0, 1, 100)\n\nfig = plt.figure(figsize=(8, 4))\nplt.plot(xpoints, scipy.stats.beta.pdf(xpoints, a=7, b=5))\n\nplt.xlabel(\"success probability\")\nplt.ylabel(\"probability density\")\n\nText(0, 0.5, 'probability density')\n\n\n\n\n\nAs expected, the peak (maximum likelihood estimator) is at 0.6 or 6 heads / 10 flips. Meanwhile, other neighbouring values, such as 0.59 or 0.61 , are also very likely. Let us look at the case of observing 60 heads and 40 tails for 100 tosses. We will plot the density from a Beta distribution with parameter [60 + 1, 40 + 1]\n\nxpoints = np.linspace(0, 1, 100)\n\nfig = plt.figure(figsize=(8, 4))\nplt.plot(xpoints, scipy.stats.beta.pdf(xpoints, a=61, b=41))\n\nplt.xlabel(\"success probability\")\nplt.ylabel(\"probability density\")\n\nText(0, 0.5, 'probability density')\n\n\n\n\n\nThe peak is still at 0.6. However, the distribution is now much more concentrated. Previously, we observe 6 heads out of 10 tosses. Now, we observe 60 heads out of 100 tosses. Because we have more observations, our estimate for the probability of landing head = 0.6 is more confident and precise.\nThe uninformative case with no observation [0 head, 0 tail] corresponds to the parameter [1, 1]. The density is spread uniformly for all possible value of the success probability, between 0 and 1.\n\nxpoints = np.linspace(0, 1, 100)\n\nfig = plt.figure(figsize=(8, 4))\nplt.plot(xpoints, scipy.stats.beta.pdf(xpoints, a=1, b=1))\n\nplt.xlabel(\"success probability\")\nplt.ylabel(\"probability density\")\n\nText(0, 0.5, 'probability density')"
  },
  {
    "objectID": "posts/probability_distribution/2023_05_16_probability_distribution.html#multinomial",
    "href": "posts/probability_distribution/2023_05_16_probability_distribution.html#multinomial",
    "title": "Intuition for a few common probability ditributions",
    "section": "Multinomial",
    "text": "Multinomial\nUp until now, our value of interest is binary: either head or tail, success or failure, 0 or 1.\nIf we know the probability of success or landing head, we can generate counts of head by sampling from the binomial distribution.\nParameter -&gt; Data : Binomial.\nConversely, if we have counts of head, we can estimate the likelihood for the probability of success.\nData -&gt; Parameter : Beta\nWhen the outcome has more than 2 dimensions, the multivariate generalization of Binomial is Multinomial (Categorical, Discrete) distribution, and for Beta we have Dirichlet distribution.\nInstead of flipping coins, the game is now rolling dices. Suppose we have a fair dice with equal probability of landing on each side = 1/6. Let us simulate 60 rolls with this dice. We repeat this experiment 10 times, and plot the counts of landing on each side.\n\ndice_rolls = np.random.multinomial(n=60, pvals=[1/6.]*6, size=10)\n\nfig = plt.figure(figsize=(8,6))\nsns.swarmplot(x=np.repeat(range(1, 7), dice_rolls.shape[0]), y=dice_rolls.transpose().flatten(), s=10)\n\nplt.ylabel(\"count\")\nplt.xlabel(\"dice side number\")\n\nText(0.5, 0, 'dice side number')\n\n\n\n\n\nWe expect to see around 10 counts = 1/6 * 60 of each side from 1 to 6. We also observe more extreme values such as 17 and 4. As with the binomial distribution, we know the parameter: the probability of landing on each side of the dice, from 1 to 6. We can sample the counts of landing on each side from this model.\nParameter -&gt; Data : Multinomial\nMultinomial is the generalization from binary to multivariate of Binomial distribution, from 2-side coin to 6-side dice."
  },
  {
    "objectID": "posts/probability_distribution/2023_05_16_probability_distribution.html#dirichlet",
    "href": "posts/probability_distribution/2023_05_16_probability_distribution.html#dirichlet",
    "title": "Intuition for a few common probability ditributions",
    "section": "Dirichlet",
    "text": "Dirichlet\nIf we have counts of landing on each side, and wish to estimate the likelihood for the probability of each side, we use the Dirichlet distribution.\nData -&gt; Parameter : Dirichlet\nTo facilitate visualization, we will consider a special dice. This special dice has the number 1 on 2 sides, number 2 on 2 sides and number 3 on the remaining 2 sides. Thus, we can describe this dice by 3 numbers, corresponding to the probability of landing on 1, 2 or 3.\nThese 3 numbers must form a valid probability distribution. Specifically, each component must be a real number between 0 and 1. Moreover, their sum must be 1. Let us denote by \\(p_1\\) the probability of landing on 1, \\(p_2\\) the probability of landing on 2, \\(p_3\\) the probability of landing on 3.\n\\[\n\\begin{aligned}\n0 \\leq p_1 \\leq 1 \\\\\n0 \\leq p_2 \\leq 1 \\\\\n0 \\leq p_3 \\leq 1 \\\\\np_1 + p_2 + p_3 = 1\n\\end{aligned}\n\\]\nWith n = the dimension of the outcome, the more general form of these conditions is:\n\\[\n\\begin{aligned}\n0 \\leq p_i \\leq 1 ; \\forall i = 1, 2, 3, ..., n \\\\\n\\sum_i^n p_i = 1\n\\end{aligned}\n\\]\nThese conditions define what is called probability simplex. It is simply all configurations which can form a valid probability distribution. In the 3 dimensional case, the simplex is a triangle. Let us plot some examples.\n\n\n#\n\n#functions for computing and plotting dirichlet distribution\n\ndef xy2bc(xy, corners, midpoints, tol=1.e-3):\n    '''Converts 2D Cartesian coordinates to barycentric.'''\n    s = [(corners[i] - midpoints[i]).dot(xy - midpoints[i]) / 0.75 \\\n         for i in range(3)]\n    return np.clip(s, tol, 1.0 - tol)\n\ndef dirichlet_pdf(pointarray, alpha):\n    # x=x/np.sum(x) : enforce simplex constraint\n    density = np.array([scipy.stats.dirichlet.pdf(x=x/np.sum(x), alpha=alpha) for x in pointarray])\n    return density\n\ndef draw_dirichlet_contours(density, trimesh, xymesh, figsize=(6,6), nlevels=100, **kwargs):\n    fig = plt.figure(figsize=figsize)\n    plt.tricontourf(trimesh, density, nlevels, **kwargs)\n    plt.axis('equal')\n    plt.xlim(0, 1)\n    plt.ylim(0, 0.75**0.5)\n    plt.axis('off')\n    plt.colorbar()\n\n    return fig\n\ndef plot_surface(trimesh, density, figsize=(8,6)):\n    fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"}, figsize=figsize)\n    surf = ax.plot_trisurf(trimesh.x, trimesh.y, density, cmap=\"viridis\")\n    fig.colorbar(surf, shrink=0.5, aspect=5)\n    ax.set_xticks([])\n    ax.set_yticks([])\n\n    return fig\n\n# http://christianherta.de/lehre/dataScience/bayesian/Multinomial-Dirichlet.slides.php\n\n\ncorners = np.array([[0, 0], [1, 0], [0.5, 0.75**0.5]])\ntriangle = tri.Triangulation(corners[:, 0], corners[:, 1])\nmidpoints = [(corners[(i + 1) % 3] + corners[(i + 2) % 3]) / 2.0 \\\n             for i in range(3)]\n\nrefiner = tri.UniformTriRefiner(triangle)\ntrimesh = refiner.refine_triangulation(subdiv=4)\n\nxymesh = np.array([xy2bc(xy, corners, midpoints) for xy in zip(trimesh.x, trimesh.y)])\n\n\ndensity = dirichlet_pdf(xymesh, [1, 1, 1])\nfig = draw_dirichlet_contours(density, trimesh, xymesh)\n\n\n\n\n\nfig = plot_surface(trimesh, density)\n\n\n\n\nThe 1st plot is a contour plot. The triangle is the probability simplex in 3 dimensions. In other words, it represent all combination of a trio \\([p_1, p_2, p_3]\\), which lies between 0 and 1 and sums up to 1. The color denotes density.\nThe corner of the triangle are extremely skewed configuration:\n\\[\n\\begin{aligned}\n[p_1 = 1, p_2 = 0, p_3 = 0] \\\\\n[p_1 = 0, p_2 = 1, p_3 = 0] \\\\\n[p_1 = 0, p_2 = 0, p_3 = 1]\n\\end{aligned}\n\\]\nThe center of the triangle is the equal probability configuration: \\[[p_1 = 1/3, p_2 = 1/3, p_3 = 1/3]\\]\nThe 2nd plot is a surface plot, or a 3 dimensional representation of the 2-dimensional contour plot. The probability simplex triangle lies in the plane at the bottom. Density is represented by color and altitude.\nAbove is a Dirichlet distribution with parameter [1, 1, 1]. This is the uninformative case with no obsersation, as with the Beta distribution we saw earlier. The density is spread out uniformly on the whole simplex triangle.\n\ndensity = dirichlet_pdf(xymesh, [2, 2, 2])\nfig = draw_dirichlet_contours(density, trimesh, xymesh)\n\n\n\n\n\nfig = plot_surface(trimesh, density)\n\n\n\n\nThis is a Dirichlet density with parameter [2, 2, 2]. In this case, we observe 1 count of landing on 1, 1 count of landing on 2, and 1 count of landing on 3 with our special dice. Reasonably, the most likely parameters are at the center of the triangle, the equal probability configuration \\([p_1 = 1/3, p_2 = 1/3, p_3 = 1/3]\\). The density decreases gradually as we move from the center to the corners.\n\ndensity = dirichlet_pdf(xymesh, [31, 21, 11])\nfig = draw_dirichlet_contours(density, trimesh, xymesh)\n# fig.savefig(\"dirichlet-density.svg\")\n\n\n\n\n\nfig = plot_surface(trimesh, density)\n\n\n\n\nIn this example, we observe 30 counts of 1, 20 counts of 2 and 10 counts of 3. The density is thus most strongly skewed toward the left corner, corresponding to the value 1, where \\([p_1 = 1, p_2 = 0, p_3 = 0]\\). Compared to the previous case of 1 count for each number [1, 1, 1], the density is also much more concentrated. We also saw this with the Beta distribution. The more observation we have, the more confident and precise the estimate for the probability will be."
  },
  {
    "objectID": "posts/probability_distribution/2023_05_16_probability_distribution.html#summary",
    "href": "posts/probability_distribution/2023_05_16_probability_distribution.html#summary",
    "title": "Intuition for a few common probability ditributions",
    "section": "Summary",
    "text": "Summary\nHere is an anecdotal way of thinking about these 4 distributions:\nBinomial (Bernoulli): a manufacturer gives us a coin, and tells us its probability of landing on head. We flip the coin, and expect the count of head to be close to the theoretical value.\nMultinomial: a manufacturer gives us a dice, and tells us the probability of landing on each side. We roll the dice, and expect to see the count for each side to be close to the theoretical value.\nBeta: we ask a manufacturer to make a coin with a specific configuration. For example, we want to get 6 heads out of 10 flips.\nDirichlet: we ask a manufacturer to make a dice with a specific configuration. For example, we want to get 1 counts for each of the sides 1, 2, 3, and 2 counts for each of the sides 4, 5, 6, for a total of 9 rolls.\nFor Beta and Dirichlet, the manufacturer will be sensitive to the degree of precision that we require. When we ask for 60 heads out of 100 flips, we expect the probability of landing head to be more concentrated around 0.6, compared to 6 head out of 10.\nHere is a recapitulative table summarizing the relationships between these 4 distributions\n\n\n\n\nParameter -&gt; Data\nData -&gt; Parameter\n\n\n\n\nBinary (coin)\nBinomial\nBeta\n\n\nMultivariate (dice)\nMultinomial\nDirichlet"
  },
  {
    "objectID": "posts/probability_distribution/2023_05_16_probability_distribution.html#sum-of-random-variables",
    "href": "posts/probability_distribution/2023_05_16_probability_distribution.html#sum-of-random-variables",
    "title": "Intuition for a few common probability ditributions",
    "section": "Sum of random variables",
    "text": "Sum of random variables\nThis final section is small digression. Here is an intuitive explanation for the Central Limit Theorem: with large enough sample size, the sum (or mean) of independent samples drawn from an identical distribution of any kind, including non Gaussian, will approach a Gaussian shape.\nLet us look at a simple case of summing 2 uniform random variables. For a more concrete example, we will examine the sum of 2 fair dices. The theoretical result is:\n2 = 1 + 1 =&gt; probability = 1 / 36\n3 = 2 + 1 =&gt; probability = 2 / 36\n4 = 1 + 3 = 2 + 2 =&gt; probability = 3 / 36\n5 = 1 + 4 = 2 + 3 =&gt; probability = 4 / 36\n6 = 1 + 5 = 2 + 4 = 3 + 3 =&gt; probability = 5 / 36\n7 = 6 + 1 = 2 + 5 = 3 + 4 =&gt; probability = 6 / 36\n8 = 2 + 6 = 3 + 5 = 4 + 4 =&gt; probability = 5 / 36\n9 = 6 + 3 = 5 + 4 =&gt; probability = 4 / 36\n10 = 6 + 4 = 5 + 5 =&gt; probability = 3 / 36\n11 = 5 + 6 =&gt; probability = 2 / 36\n12 = 6 + 6 =&gt; probability = 1 / 36\n\nproba_sum2dices = np.concatenate([np.arange(1, 7), np.arange(5, 0, -1)]) / 36\n\nfig = plt.figure(figsize=(6, 6))\nplt.bar(np.arange(2, 13), proba_sum2dices )\nplt.ylabel(\"probability\")\nplt.xlabel(\"sum of 2 dices\")\n\nText(0.5, 0, 'sum of 2 dices')\n\n\n\n\n\nWe can already notice the bell shaped curve, even though the initial uniform distribution is very different from a gausian distribution. When summing 2 dices, number 7 has the widest range of possible combinations. Thus it is the most likely values. The probability decreases symmetrically, when we move away from the mode to the 2 extremes.\nNext time when you question why the Central Limit Theorem holds in practice, perhaps it may help to think about the sum of 2 dices."
  },
  {
    "objectID": "posts/constrained_optimization/2023_07_22_constrained_optimization.html",
    "href": "posts/constrained_optimization/2023_07_22_constrained_optimization.html",
    "title": "Constrained optimization and Karush Kuhn Tucker condition (KKT condition)",
    "section": "",
    "text": "Suppose we are minimizing a function \\(f(x)\\), with contour line shown in the illustration below. These lines denotes points with the same function value. In the mountain landscape analogy, these line are projection of the altitude on a flat surface such as a map.\nThe shaded gray region represents a constraint that the solution must satisfy: \\(g_i(x) &lt; 0\\).\nThe KKT conditions allows us to narrow down the number of potential optimum candidates. Among these conditions, the most interesting one is complementary slackness. This condition states that, either the constraint have no impact on the original optimum, or the optimum lies on the boundary of the constraint.\nThis post aims to explain the intuition behind teh KKT conditions. We will start by defining the problem and stating the conditions in a somewhat theoretical manner. Then, we will come back to the illustration and see why the conditions make sense.\nImage from https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions"
  },
  {
    "objectID": "posts/constrained_optimization/2023_07_22_constrained_optimization.html#constrained-optimization",
    "href": "posts/constrained_optimization/2023_07_22_constrained_optimization.html#constrained-optimization",
    "title": "Constrained optimization and Karush Kuhn Tucker condition (KKT condition)",
    "section": "Constrained optimization",
    "text": "Constrained optimization\nWe start by writing down the problem of minimizing a function, with m inequality constraints indexed by i, and l equality contraints indexed by j.\n\\[\\begin{align}\n\\min_x f(x)\n\\\\\n\\text{under constraint:}\n\\\\\ng_i(x) \\leq 0 ; \\forall i = 1...m\n\\\\\nh_j(x) = 0 ; \\forall j = 1...l\n\\end{align}\\]\nIt seems very cumbersome to carry this whole bunch of constraints around while looking for the optimum. To simplify the task, let’s incorporate the constraints into the objective function. Instead of ensuring the constraint satisfaction, we will impose a penalty for constraint violation. This technique is known as Lagrangian relaxation.\nWe will refer to this new objective with the penalty incorporated in as the Lagrangian function:\n\\[\\begin{align}\n\\min L(x, \\mu, \\lambda) = \\underbrace{f(x)}_{\\text{original objective}} + \\underbrace{\\sum_{i=1}^m \\mu_i g_i(x)}_{\\text{inequality penalty}} + \\underbrace{\\sum_{j=1}^l \\lambda_j h_j(x)}_{\\text{equality penalty}}\n\\\\\n\\text{under condition:}\n\\\\\n\\mu_i \\geq 0; \\forall i = 1...m\n\\end{align}\\]\nWhat we did was adding all the constraints into the objective, with a multiplier in front. These multipliers become additional variables (dual variables) of the new objective function. The next section will explain why this becomes a penalty for constraint violation."
  },
  {
    "objectID": "posts/constrained_optimization/2023_07_22_constrained_optimization.html#equality-constraint-penalty",
    "href": "posts/constrained_optimization/2023_07_22_constrained_optimization.html#equality-constraint-penalty",
    "title": "Constrained optimization and Karush Kuhn Tucker condition (KKT condition)",
    "section": "Equality constraint penalty",
    "text": "Equality constraint penalty\nLet’s look at the second new term coming from the equality constraints: \\[\\begin{align}\n\\sum_{j=1}^l \\lambda_j h_j(x) = \\lambda_1 h_1(x) + ... + \\lambda_l h_l(x)\n\\end{align}\\] The original inequality constraints is: \\[ h_j(x) = 0 ; j = 1...l \\] If an equality constraint is violated, then: \\[ h_j(x) \\neq 0 \\]\nRemember that we are minimizing the Lagrangian function, in other words, the original objective function with incorporated constraint penalty:\n\\[\\begin{align}\n\\min L(x, \\mu, \\lambda) = \\underbrace{f(x)}_{\\text{original objective}} + \\underbrace{\\sum_{i=1}^m \\mu_i g_i(x)}_{\\text{inequality penalty}} + \\color{red}{ \\underbrace{\\sum_{j=1}^l \\lambda_j h_j(x)}_{\\text{equality penalty}} }\n\\end{align}\\]\nWhat happens when an equality constraint is violated? Since \\(h_j(x) \\neq 0\\), there is an easy way to cheat with the minimization objective.\nIf \\(h_j(x) &gt; 0\\), set \\(\\lambda_j = - \\infty\\).\nIf \\(h_j(x) &lt; 0\\), set \\(\\lambda_j = + \\infty\\).\n\\(=&gt; \\lambda_j h_j(x) = - \\infty\\), in either case.\n\\[\\begin{align}\n\\min L(x, \\mu, \\lambda) = \\underbrace{f(x)}_{\\text{original objective}} + \\underbrace{\\sum_{i=1}^m \\mu_i g_i(x)}_{\\text{inequality penalty}} + \\color{red}{ \\underbrace{(-\\infty)}_{\\text{equality penalty}} }\n\\end{align}\\]\nThe equality penalty blows up the minimization objective. It messes up everything and the minimum is now ill-defined. That is the penalty for violating the constraint. We had better respect it or everything will fall apart."
  },
  {
    "objectID": "posts/constrained_optimization/2023_07_22_constrained_optimization.html#minus-infinity-minimum",
    "href": "posts/constrained_optimization/2023_07_22_constrained_optimization.html#minus-infinity-minimum",
    "title": "Constrained optimization and Karush Kuhn Tucker condition (KKT condition)",
    "section": "Minus infinity minimum",
    "text": "Minus infinity minimum\nIn practice, I am unable to find any example for the minus infinity minimum case. There will be physical constraint limiting the minimal value. For instance, if I am trying to dive as deep as possible into the ocean, then what I am minimizing is very simple:\n\\[\\min_x f(x) = x\\]\nHere, \\(x\\) is my negative altitude below sea level. That value is limited by several factors, for example: the altitude of the seabed, or the amount of oxygen that I can carry with me for a diving trip. There is no way of violating these constraints.\nDiving to altitude minus infinity only exists in theory, not in practice.\nThus, the equality constraint penalty that we set up in the Langragian function has fulfilled its duty. When the equality constraint is violated, the penalty will give us back the insensible minimum value of minus infinity."
  },
  {
    "objectID": "posts/constrained_optimization/2023_07_22_constrained_optimization.html#inequality-constraint-penalty",
    "href": "posts/constrained_optimization/2023_07_22_constrained_optimization.html#inequality-constraint-penalty",
    "title": "Constrained optimization and Karush Kuhn Tucker condition (KKT condition)",
    "section": "Inequality constraint penalty",
    "text": "Inequality constraint penalty\nLet’s examine the first new term coming from the inequality constraints: \\[\\begin{align}\n\\sum_{i=1}^m \\mu_i g_i(x)  = \\mu_1 g_1(x) + ... + \\mu_m g_m(x)\n\\end{align}\\] The original inequality constraints is: \\[ g_i(x) \\leq 0 ; i = 1...m \\] If an inequality constraint is violated, then: \\[ g_i(x) &gt; 0 \\] We impose that the multipliers must be positive or zero: \\[ \\mu_i \\geq 0 \\] The product will then be positive or zero: \\[ \\mu_i g_i(x) \\geq 0 \\] As we add these terms into the minimization objective, they will become a penalty by increasing the minimization objective value if the inequality constraints are violated.\n\\[\\begin{align}\n\\min L(x, \\mu, \\lambda) = \\underbrace{f(x)}_{\\text{original objective}} + \\color{red} { \\underbrace{\\sum_{i=1}^m \\mu_i g_i(x)}_{\\text{inequality penalty}} } +  \\underbrace{\\sum_{j=1}^l \\lambda_j h_j(x)}_{\\text{equality penalty}}\n\\end{align}\\]\nBut the cheat with infinity can happens here too. When the constraint is satisfied but not saturated, in other words: \\[ g_i(x) &lt; 0 \\] (instead of \\(g_i(x) \\leq 0\\))\nBy setting \\(\\mu_i = + \\infty\\), the product will be minus infinity: \\[ \\mu_i g_i(x) = - \\infty \\]\nThe minimum will be ill-defined, as with the cheat for equality constraint:\n\\[\\begin{align}\n\\min L(x, \\mu, \\lambda) = \\underbrace{f(x)}_{\\text{original objective}} + \\color{red} { \\underbrace{ (- \\infty) }_{\\text{inequality penalty}} } +  \\underbrace{\\sum_{j=1}^l \\lambda_j h_j(x)}_{\\text{equality penalty}}\n\\end{align}\\]\nAmong the KKT conditions, the complementary slackness condition demands that the product \\(\\mu_i g_i(x) = 0\\), so the minus infinity case never happens.\nNext, we will get to the KKT conditions themselves and clarify everything."
  },
  {
    "objectID": "posts/constrained_optimization/2023_07_22_constrained_optimization.html#optimality-necessary-conditions",
    "href": "posts/constrained_optimization/2023_07_22_constrained_optimization.html#optimality-necessary-conditions",
    "title": "Constrained optimization and Karush Kuhn Tucker condition (KKT condition)",
    "section": "Optimality necessary conditions",
    "text": "Optimality necessary conditions\nWe rewrite the objective in vector format to simplify notation: \\[\\begin{align}\n\\min L(x, \\mu, \\lambda) = f(x) + \\mu^T g(x) + \\lambda^T h(x)\n\\\\ \\\\\n\\mu = \\begin{bmatrix} \\mu_1 \\\\ \\vdots \\\\ \\mu_m \\end{bmatrix} ;\ng(x) = \\begin{bmatrix} g_1(x) \\\\ \\vdots \\\\ g_m(x) \\end{bmatrix} ;\n\\lambda = \\begin{bmatrix} \\lambda_1 \\\\ \\vdots \\\\ \\lambda_m \\end{bmatrix} ;\nh(x) = \\begin{bmatrix} h_1 \\\\ \\vdots \\\\ h_m(x) \\end{bmatrix} ;\n\\\\ \\\\\n\\mu^T g(x) = \\mu_1 g_1(x) + ... + \\mu_m g_m(x)\n\\\\\n\\lambda^T h(x) = \\lambda_1 h_1(x) + ... + \\lambda_l h_l(x)\n\\end{align}\\]\nAn optimum of this optimization problem must satify all these conditions:\n\nStationarity:\n\n\\[ \\nabla_x L(x, \\mu, \\lambda) = \\nabla_x f(x) + \\mu^T \\nabla_x g(x) + \\lambda^T \\nabla_x h(x) = 0  \\] This is the classic optimality condition: at an optimum, the gradient must be 0. If gradient is anything other than zero, then we can find a lower objective value my moving in the opposite direction of the gradient. The optimum must be lower than all surrounding points, thus the gradient have to be zero.\n\nPrimal feasibility:\n\n\\[\\begin{align}\ng_i(x) \\leq 0 ; \\forall i = 1...m\n\\\\\nh_j(x) = 0 ; \\forall j = 1...l\n\\end{align}\\]\nJust copy and paste the original inequality and equality constraint.\n\nDual feasibility:\n\n\\[ \\mu_i \\geq 0 ; \\forall i = 1...m \\]\nAs we have seen in the inequality constraint section, this condition ensures that the penalty for violating the inequality constraints has the correct sign.\n\nComplementary slackness:\n\n\\[ \\mu_i g_i(x) = 0 ; \\forall i = 1...m \\] This condition is the key to help us solve for optimality. Each inequality constraint can only be in either one of 2 cases: \\(\\mu_i = 0\\) or \\(g_i(x) = 0\\).\nIn the first case \\(\\mu_i = 0\\), the constraint has no impact on the optimal solution. We can ignore this constraint.\nIn the second case \\(g_i(x) = 0\\), the inequality becomes equality, thus it is easier to solve. The optium lies on the boundary of the feasible region.\nLet’s illustrate complementary slackness with an example."
  },
  {
    "objectID": "posts/constrained_optimization/2023_07_22_constrained_optimization.html#complementary-slackness-illustration",
    "href": "posts/constrained_optimization/2023_07_22_constrained_optimization.html#complementary-slackness-illustration",
    "title": "Constrained optimization and Karush Kuhn Tucker condition (KKT condition)",
    "section": "Complementary slackness illustration",
    "text": "Complementary slackness illustration\nWe will take as example this simple minimization:\n\\[ \\min_{x_1, x_2} f(x_1, x_2) = x_1^2 + x_2^2 \\]\nThe global minimum is (0, 0). Next, we will plot the value and contour plot of this function around the minimum.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nx1 = np.linspace(-3, 3, 100)\nx2 = np.linspace(-3, 3, 100)\nx1, x2 = np.meshgrid(x1, x2)\ny = x1**2 + x2**2\n\n\nfig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\nsurface = ax.plot_surface(x1, x2, y, cmap=\"viridis\", linewidth=0, antialiased=False)\nax.set(xlabel='x1', ylabel='x2', zlabel='f(x)')\n\n[Text(0.5, 0, 'x1'), Text(0.5, 0.5, 'x2'), Text(0.5, 0, 'f(x)')]\n\n\n\n\n\n\ncontour = plt.contour(x1, x2, y, cmap=\"viridis\")\nplt.clabel(contour, inline=True, fontsize=10)\nplt.plot(0, 0, marker='o', color='red')\nplt.xlabel('x1')\nplt.ylabel('x2')\nplt.colorbar()\n\n&lt;matplotlib.colorbar.Colorbar at 0x7a41ac607100&gt;\n\n\n\n\n\nThe red dot in the contour plot represents the global minimum (0, 0). Now we will add an inequality constraint: \\[ g(x) =  x_1 - 1 \\leq 0 \\implies x_1 \\leq 1 \\]\nLet’s plot this constraint along with the original contour plot. We will add a gray shade to the infeasible region $ x_1 &gt; 1 $.\n\ncontour = plt.contour(x1, x2, y, cmap=\"viridis\")\nplt.clabel(contour, inline=True, fontsize=10)\nplt.plot(0, 0, marker='o', color='red')\nplt.colorbar()\nplt.xlabel('x1')\nplt.ylabel('x2')\nplt.axvspan(1, 3, color='black', alpha=0.2)\n\n&lt;matplotlib.patches.Polygon at 0x7a41ac6cd9c0&gt;\n\n\n\n\n\nIn this case, the constraint has no impact on the original optimum. Since the original minimum (0, 0) is inside the feasible region, it is still the optimum for the constrained optimization. Coming back to complementary slackness and KKT condition, the inequality multiplier \\(\\mu = 0\\).\nNow, let’s consider another constraint and plot it: \\[ g(x) =  x_1 + 1 \\leq 0 \\implies x_1 \\leq -1 \\]\n\ncontour = plt.contour(x1, x2, y, cmap=\"viridis\")\nplt.clabel(contour, inline=True, fontsize=10)\nplt.plot(-1, 0, marker='o', color='red')\nplt.colorbar()\nplt.xlabel('x1')\nplt.ylabel('x2')\nplt.axvspan(-1, 3, color='black', alpha=0.2)\n\n&lt;matplotlib.patches.Polygon at 0x7a41ac546080&gt;\n\n\n\n\n\nThe original minimum (0, 0) is now inside the infeasible region. Thus, it can no longer be the optimum of the constrained optimization. Complementary slackness tells us: the optimum must lies on the boundary. On the line \\(x_1 = -1\\), the point with the lowest objective function value is (-1, 0).\nIt is easy and intuive to see why (-1, 0) is the new minimum for the constrained minimization. We want to find the closest point to the original minimum (0, 0). Due to the constraint, the closest that we can get to the right is the boundary \\(x_1 = -1\\). On this line, the closest vertical point to (0, 0) is (-1, 0)."
  },
  {
    "objectID": "posts/constrained_optimization/2023_07_22_constrained_optimization.html#conclusion",
    "href": "posts/constrained_optimization/2023_07_22_constrained_optimization.html#conclusion",
    "title": "Constrained optimization and Karush Kuhn Tucker condition (KKT condition)",
    "section": "Conclusion",
    "text": "Conclusion\nThe KKT conditions helps us to solve for the optimum of a constrained optimization problem. Among these conditions, the most essential condition is complementary slackness.\nComplementary slackness may appear confusing at first, but we have seen with the illustrations that it is actually very intuitive. As optimization is about finding the extreme, we must go to the extreme of the feasible region to find the optimum. If we fail to find the optimum there, then we can ignore this constraint, because the original unconstrained optimum is unaffected by the constraint.\nTo sum up the intuition behind complementary slackness, let’s consider the diving analogy again. If I am trying to dive as deep as possible into the ocean, then I am minimizing my negative altitude below the sea level.\nIf my oxygen tank only allows me to dive 100 meters deep, then the constraint is \\[ altitude \\leq -100 \\] My minimum altitude is precisely at the boundary of what is feasible, or 100 meters deep: \\[ \\min (altitude) = -100 \\]\nMeanwhile, if the seabed where I am diving is only 5 meters deep, then the oxygen capacity constraint of diving 100 meters deep or less becomes irrelevant. The minimum altitude will always be -5. I can even dive without oxygen tank and forget about the constraint."
  }
]